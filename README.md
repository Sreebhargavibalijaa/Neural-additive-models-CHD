# Neural-additive-models-CHD

As a wide variety of AI models continue to emerge in the
machine learning research space, it has been observed that
most of the models (like transformers, deep neural nets etc.)
are the metaphorical ”black boxes”. There is one critical aspect in which these models fall short, i.e. interpretabil-
ity. As these models are not interpretable, researchers can-
not make inferences. This paper uses a recently proposed
technique to leverage the power of neural networks while
retaining interpretability. This work also conducts a com-
parative analysis of different models, across the spectrum of
complexity. We use a medical data set ’frimhingham data’
to show that the Neural Additive Models are very much in-
terpretable as well as accurate. To evaluate interpretabil-
ity, we used logistic regression as our reference and we ob-
served that the results of NAMs are in line with the results
of logistic regression.
